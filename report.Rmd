---
title: "report7"
author: "ZJH"
date: "`r Sys.time()`"
output:
  word_document: default
  html_document: 
    df_print: paged
---

```{r global_options,include=FALSE}
knitr::opts_chunk$set(cache =F,warning = FALSE,include = F)

```

# library import

```{R,include = FALSE,echo = FALSE}
set.seed(6010)
library(gvlma)
library(caret)  
library(car)
library(glmnet)  
library(GGally)
library(h2o)          # an extremely fast java-based platform
library(lmtest)
library(magrittr)
library(nnet)
library(rsample)      # data splitting 
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest       # an aggregator package for performing many machine learning models
library(pheatmap)
library(tidyverse)
library(forecast)
library(pROC)
```

# data import

```{R,include = FALSE,echo = FALSE}
set.seed(6010)
tea <- readRDS('nonglin_tea.RDS')
analyze_data <- readRDS('analyze.RDS')

#weather$acu_mean_temp <- weather$acu_mean_temp - mean(weather$acu_mean_temp)
# png('var_scatter_hist_weather_chem.png',width = 3400,height=3400)
# scatter_hist_plot(weather_chem)
# dev.off()
```

# data cleaning

```{R,include = FALSE,echo = FALSE}

numeric_dat <- analyze_data %>% select(.,-c(key,observe_ys,cultivar,observe_date:sample_label,G1L:O9L,year,temp_differ))
chem <- analyze_data %>% select(,c(polyphenol:ECG,level))
table(chem$level)
chem$level <- factor(chem$level) %>% as.numeric()
chem$level <- ifelse(chem$level>1,1,0)
test_idx <- sample(1:138,size = 30)
train_idx <- not(1:138 %in% test_idx)
chem_test <- chem[test_idx,]
chem_train <- chem[train_idx,]
chem_test_y <- chem_test$level

```

# functions include

```{R,include = FALSE,echo = FALSE}
simple_roc <- function(labels, scores){
  labels <- labels[order(scores, decreasing=TRUE)]
  data.frame(TPR=cumsum(labels)/sum(labels), FPR=cumsum(!labels)/sum(!labels), labels)
}
```

```{R,include = FALSE,echo = FALSE}
importanceplot <- function(pp=and_rf) {
  i_scores <- varImp(pp, conditional=TRUE)
  #Gathering rownames in 'var'  and converting it to the factor
  #to provide 'fill' parameter for the bar chart.
  i_scores <- i_scores %>% tibble::rownames_to_column("var")
  i_scores$var<- i_scores$var %>% as.factor()

  #Plotting the bar and polar charts for comparing variables
  i_bar <- ggplot(data = i_scores) +
    geom_bar(
      stat = "identity",#it leaves the data without count and bin
      mapping = aes(x = var, y=Overall, fill = var),
      show.legend = FALSE,
      width = 1
    ) +
    labs(x = NULL, y = NULL)
  return(i_bar + coord_flip() + theme_minimal())
  }

```

```{R,include = FALSE,echo = FALSE}
cross_validation <- function(y_pred,ytest,prob=0.75){
  y_pred <- ifelse(y_pred<=prob,'B','C')
  ytest <- if(is.null(dim(ytest))){ytest}else{ytest$level}
  print(table(y_pred,ytest))
}
```

```{R,include = FALSE,echo = FALSE}
rf_this <- function(y,dat,train, test,train_idx, test_idx) {
  model_formula <- formula(paste0(y,'~.'))
  xtrain <- model.matrix(model_formula, train)[,-1]
  ytrain <- train[y]
  ytest <- test[y]
  xtest <- model.matrix(model_formula, test)[,-1] %>%  as_tibble()%>% as.matrix()
  and_rf <- randomForest(model_formula, data = dat[train_idx,])
  print(and_rf)
  print(importanceplot(and_rf))
}

scatter_hist_plot <- function(dat) {
  n = length(colnames(dat))
  for (i in 1:n) 
    {
     for(j in 1:n)
      {
        if(i==j)
        {
          test <- ggplot(data=dat,aes(get(colnames(dat[i]))))+
            geom_histogram(bins = 30,fill = 'darkgreen')+
            labs(x=paste(i,colnames(dat[i]),y=NULL))
        }
        else
        {
          test <- ggplot(data=dat,aes(get(colnames(dat[i])),get(colnames(dat[j]))),colour='green')+
            geom_point()+
            labs(x=paste(i,colnames(dat[i])),y=paste(j,colnames(dat[j])))
        }
        print(test,vp=viewport(width=1/n, 
                               height=1/n,
                               x=((i-1)%%n)/n+1/(2*n),
                               y=((j-1)%%n)/n+1/(2*n)
                               )
              )
      }
    }
}
```

```{R,include = FALSE,echo = FALSE}
print_model = function(type,model,test,test_y)
  {
    print(type)
    print(summary(model))
    y_pred <- predict(model,test)
    #cross_validation(y_pred,test_y,prob=0.75)
    #print(accuracy(y_pred,test_y))
  }
for_back_selection <- function(y,train,test, test_y,model_chosen, full_model = '') {
  
  # 1.建立空的線性迴歸(只有截距項)
  full_model <- if(full_model==''){formula(paste0(y,'~.'))}else{full_model}
  null <- model_chosen(formula(paste0(y,'~1')) ,data=train,family=binomial())
  full <- model_chosen(full_model ,data=train,family=binomial())
  
  # 2.使用step()，一個一個把變數丟進去
  forward.lm = step(null, 
                    # 從空模型開始，一個一個丟變數，
                    # 最大不會超過完整的線性迴歸
                    # (一定要加上界 upper=full，不可以不加) 
                    scope=list(lower=null, upper=full), 
                    direction="forward",
                    trace = 0)
  backward.lm = step(full, 
                    # 這裡可以加下界(lower=null)，也可以不加
                    scope = list(upper=full), 
                    direction="backward",
                    trace = 0) 
  print('==============')
  if(is.null(summary(forward.lm)$adj.r.squared))
    {
      if(summary(forward.lm)$aic<summary(backward.lm)$aic)
        {
        print_model('forward',forward.lm,test,test_y)
        select_model = forward.lm
        }
      else
      {
      print_model('backward',backward.lm,test,test_y)
      select_model = backward.lm
      }
    }
    else
    {
      if(summary(forward.lm)$adj.r.squared>summary(backward.lm)$adj.r.squared)
      {
        print_model('forward',forward.lm,test,test_y)
        select_model = forward.lm
        }
      else
      {
      print_model('backward',backward.lm,test,test_y)
      select_model = backward.lm
      }
    }
  if(class(select_model)=='lm')
    {
    print(summary(gvlma(select_model)))
    }
  print('==============')
  
  return(select_model)
}

```

```{r,include = FALSE,echo = FALSE}
lasso_and_ridge <- function(dat,lasso_1_rigde_0,lm_model,family_type,yname,train,test,test_y)
{
  fnction_name <- ifelse(lasso_1_rigde_0==1,'lasso','ridge')
  print(fnction_name)
  y_idx <- which(colnames(dat)==yname)
  # lasso & ridge for dat ----
  model_step = glmnet(x = as.matrix(dat[, -y_idx]), 
                 y = dat[, y_idx], 
                 alpha = lasso_1_rigde_0,
                 family = family_type)
  
  #plot(model_step, xvar='lambda', main=fnction_name)
  cv.model_step = cv.glmnet(x = as.matrix(dat[, -y_idx]), 
                       y = dat[, y_idx], 
                       alpha = lasso_1_rigde_0,  # lasso
                       family = family_type)
  best.lambda_model_step = cv.model_step$lambda.min
  #abline(v=log(best.lambda_model_step), col="blue", lty=5.5 )

    
    # 觀察哪些變數被挑選出來，其係數不為 0的那些
    
  # 經由 cv 的手法，評估每個模型在不同 lambda 下 
  # 的 cvm(mean cross-validated error)

  # 評估每個模型的 cvm(mean cross-validated error)後
  # 取最小 cvm 模型所對應的 lambda
    select.ind = which(coef(cv.model_step, s = "lambda.min") != 0)
    select.ind = select.ind[-1]-1 # remove `Intercept` and 平移剩下的ind
    select.varialbes = colnames(dat)[select.ind]
    model_formula <- formula(paste0(yname,'~.'))
    model.lm <- lm_model(model_formula, family = family_type,data = train[, c(select.varialbes, yname)])
    print(summary(model.lm))
    model.test <- predict(model.lm,test)
    cross_validation(model.test,test_y)
}
```

```{r,include = FALSE,echo = FALSE}
model_gen_for_power_one <- function(data,power,y_name)
  {
    target_idx <- which(colnames(data)==y_name)
    target <- colnames(data)[-target_idx]
    container <- ''
    for(j in power){
    for (i in 1:length(target))
      {
          if(length(target)==i)
            {
              container <- paste0(container,
                                  'I(',target[i],'^',j,')')
              break
            }
        container <- paste0(container,
                            'I(',target[i],'^',j,')','+')
      }
    }
    return(container)
}

model_gen <- function(data,power,y_name){
  container <- ''
  counter=0
  for(i in power)
  {
    counter = counter+1
   if(counter==length(power))
    {
      container <- paste0(container,
                          model_gen_for_power_one(data,
                                                  i,
                                                  y_name))
    }
  else
    {
      container <- paste0(container,
                          model_gen_for_power_one(data,
                                                  i,y_name),
                          '+')
    }
  }
  
  return(paste0(y_name,'~',container))
}

```

```{r,include = FALSE,echo = FALSE}
get_weather_data <- function(chem_target)
{
  weather <-  analyze_data %>% select(.,c(chem_target ,acu_mean_temp:Solar_rad_MJM2))
  return(weather)
}
```

```{r,include = FALSE,echo = FALSE}
# we pause using IU becuause it may have some bug , and it is very severe when sample size is small
roc_IU_idx <- function(roc)
{
  IU <- abs(roc$sensitivities-roc$auc) + abs(roc$specificities-roc$auc)
  best_IU <- min(IU)
  idx <-  which(IU==best_IU)
  return(idx)
}
roc_ER_idx <- function(roc)
{
  ER <- abs(1-roc$sensitivities)^2 + abs(1-roc$specificities)^2
  best_ER <- min(ER)
  idx <-  which(ER==best_ER)
  return(idx)
}

print_roc <- function(test_y,test,model)
{
  pred_y <- predict(model,test)
  model_roc <- roc(test_y,pred_y,level=c(control = 0, case = 1),direction = 'auto')
  print(model_roc)
  best_idx <- roc_ER_idx(model_roc)
  x <- round(model_roc$specificities[best_idx],3)
  y <- round(model_roc$sensitivities[best_idx],3)
  print(paste0('cut_point:',round(model_roc$thresholds[best_idx],3)))
  print(paste0('Sensetivity:',y))
  print(paste0('Specificity:',x))
  title <-paste(as.character(model$formula)[2],'~',as.character(model$formula)[3])
  plot(model_roc,main = title)
  points(x=x,y= y)
  text(x = x,y=y+0.1,labels = paste('(',x,',',y,')'))
  text(x=0.2,y=0.2,labels = paste(model_roc$auc))
}
```

# data explore

season , tea type , cultivar are not balanced included

```{R}
ggplot(data=tea,aes(observe_ys,tea_kind,color=cultivar))+
  geom_point()+
  facet_wrap(~cultivar)
```

cutting height is not equal each time

```{R}
ggplot(data=tea,aes(observe_ys,cutting_height,color=cultivar))+
  geom_point()+
  facet_wrap(~cultivar)
```

some times seems linear but most of time is not , when split in to different cultivar

```{R}
ggplot(data=tea,aes(duration_oc,cutting_height,color=cultivar))+
  geom_point()+
  facet_wrap(~cultivar)

ggplot(data=analyze_data,aes(acu_mean_temp,season,color=cultivar))+
  geom_point()+
  facet_wrap(~season)
```

use lasso to choose var in weather and plant phenotypes

```{r}
set.seed(6010)
train_idx <- sample(1:138,108)
test_idx <- !(1:138 %in%train_idx)
train <- numeric_dat[train_idx,]
test <- numeric_dat[test_idx,]
xtrain <- model.matrix(polyphenol~., train)[,-1]
ytrain <- train$polyphenol

ytest <- test$polyphenol
xtest <- model.matrix(polyphenol~., test)[,-1]
season7th <- 0
xtest <- cbind(xtest,season7th)
lambdas_to_try <- 10^seq(-3, 7, length.out = 100)
lasso_cv <- cv.glmnet(xtrain, ytrain, alpha = 1, lambda = lambdas_to_try)
plot(lasso_cv)
best_lambda_lasso <- lasso_cv$lambda.min
lasso_mod <- glmnet(xtrain, ytrain, alpha = 1, lambda = best_lambda_lasso)
print(lasso_mod)
```

# level\~chem\*

## var selection

```{r}
pr_chem <- prcomp(scale(chem) )
biplot(pr_chem)
```

### random forest ver.

```{r}
xtrain <- model.matrix(level~., train)[,-1]
ytrain <- train$level
ytest <- test$level
xtest <- model.matrix(level~., test)[,-1] %>%  as_tibble() %>% as.matrix()
data_level <- analyze_data %>% select(., c(polyphenol:total_catechins,level))
data_level$level <- factor(data_level$level)
and_rf <- randomForest(level~., data = data_level[train_idx,])
and_pred <- predict(and_rf,data_level[test_idx,])
table(data_level[test_idx,]$level,and_pred)
importanceplot(and_rf)

```

### forward and backward selection

```{r,include = T}
for_back_selection('level',chem_train,chem_test,chem_test_y,glm)
```

### lasso-ridge regression ver.

```{r}
lasso_and_ridge(chem,1,lm_model = glm,family_type = binomial,yname = 'level',train = chem_train,test= chem_test,test_y = chem_test_y)
```

## var selection after data log transform

```{r}
chem <- analyze_data %>% select(,c(polyphenol:ECG,level))
chem$level <- factor(chem$level) %>% as.numeric()
chem$level <- ifelse(chem$level>1,1,0)

chem[,1:12] <- log(chem[,1:12])
chem[,6] <- ifelse(is.infinite(chem[,6]),-6,chem[,6])
test_idx <- sample(1:138,size = 30)
train_idx <- not(1:138 %in% test_idx)
chem_test <- chem[test_idx,]
chem_train <- chem[train_idx,]
chem_test_y <- chem_test$level
```

### for-back- logistic

```{r,include = T}
level_for_back_model <- for_back_selection('level',chem_train,chem_test,chem_test_y,glm)
```

### lasso-ridge

```{R}
lasso_and_ridge(chem,lasso_1_rigde_0 = 1,lm_model = glm,'binomial','level',chem_train,chem_test,chem_test_y)
```

```{R}
lasso_and_ridge(chem,lasso_1_rigde_0 = 0,lm_model = glm,'binomial','level',chem_train,chem_test,chem_test_y)
```

## parameter tuning

比較過後backward selection 原來的模型AUC 最大

Goal: 定義各 chem 在各條件下對應的 level

```{r,include = T}
all_glm <- glm(level_for_back_model$formula,family = binomial(),data = chem_train)
print(summary(all_glm))
print_roc(chem_test_y,chem_test,all_glm)
```

```{r,include = F}
num_var <- length(attr(all_glm$terms,'term.labels'))
for (model_idx in 1:num_var) {
  variables_names <- rownames(attr(all_glm$terms[model_idx],'factors'))
  model_formula <- formula(paste(variables_names[1],'~',variables_names[2]))
  
  single_model <- glm(model_formula,family = binomial(),data = chem_train)
  print_roc(chem_test_y,chem_test,single_model)
  print(variables_names[2])
}
```

# chem\*\~weather\*

```{r}
chem_target <- attr(all_glm$terms,"term.labels")
```

## random forest to chem\~weather

use randomforest to estimate chem\*\~weather

```{r}
set.seed(6010)
test_idx <- sample(1:138,size = 30)
train_idx <- not(1:138 %in% test_idx)

for (y_name in chem_target)
{
  print(y_name)
  weather <- get_weather_data(y_name)
  weather_test <- weather[test_idx,]
  weather_train <- weather[train_idx,]
  weather_test_y <- weather_test[y_name]
  rf_this(y_name,weather,weather_train,weather_test,train_idx,test_idx)
  print('===============')
}
```

## for -back

total model

```{r,include=TRUE}
dat <- analyze_data %>% select(.,polyphenol:Solar_rad_MJM2)
model_value <- t(all_glm$coefficients[-1]*t(dat[names(all_glm$coefficients[-1])])) %>% rowSums()
model_value <- model_value+all_glm$coefficients[1]
dat$model_value <- model_value
dat_train <- dat[train_idx,]
dat_test <- dat[test_idx,]

full_model <- formula('model_value~temp_differ+acu_mean_temp+rain*growth_mean_temp*RH+Solar_rad_H*Solar_rad_MJM2')
  for_back_selection(y_name,dat_train,dat_test,dat_train$model_value,lm,full_model = full_model )
```

each chem

```{r,include = T}

for (y_name in chem_target)
{
  print(y_name)
  weather <- get_weather_data(y_name)
  weather_test <- weather[test_idx,]
  weather_train <- weather[train_idx,]
  weather_test_y <- weather_test[y_name]
  full_model <- formula(paste0(y_name,'~','temp_differ+acu_mean_temp+rain*growth_mean_temp*RH+Solar_rad_H*Solar_rad_MJM2'))
  for_back_selection(y_name,weather_train,weather_test,weather_test_y,lm,full_model = full_model )
  print('===============')
}
```

## var selection base on quadratic and interaction

Since the R\^2 is low , We try this,but the result is not better than main effect,hide the result.

```{r chem to level quadratic  ,include=F}
full_model <- formula(model_gen(chem,1:2,'level'))
for_back_selection('level',chem_train,chem_test,chem_test_y,glm,full_model = full_model )
```

```{r all chem to weather quadratic,include=F}

# chem_target <- colnames(chem)[1:12]
# for (y_name in chem_target)
# {
#   print(y_name)
#   weather <- get_weather_data(y_name)
#   weather_test <- weather[test_idx,]
#   weather_train <- weather[train_idx,]
#   weather_test_y <- weather_test[y_name]
#   full_model <- formula(paste0(model_gen(weather,1:2,y_name),'+(.)^2'))
#   for_back_selection(y_name,weather_train,weather_test,weather_test_y,lm,full_model = full_model )
#   print('===============')
# }
```
