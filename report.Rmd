---
title: "report"
author: "ZJH"
date: "2022/5/13"
output:
  word_document: default
  html_document: 
    df_print: paged
---

# library import

```{R,include = FALSE}

library(caret)  
library(car)
library(glmnet)  
library(GGally)
library(h2o)          # an extremely fast java-based platform
library(lmtest)
library(magrittr)
library(nnet)
library(rsample)      # data splitting 
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest       # an aggregator package for performing many machine learning models
library(pheatmap)
library(tidyverse)
library(forecast)
```

# data import

```{R,include = FALSE}
set.seed(100)
tea <- readRDS('nonglin_tea.RDS')
analyze_data <- readRDS('analyze.RDS')
weather <-  analyze_data %>% select(.,c(caffeine ,acu_mean_temp:Solar_rad_MJM2))

# png('var_scatter_hist_weather_chem.png',width = 3400,height=3400)
# scatter_hist_plot(weather_chem)
# dev.off()
```

\#data cleaning

```{r}

numeric_dat <- analyze_data %>% select(.,-c(key,observe_ys,cultivar,observe_date:sample_label,G1L:O9L,year,temp_differ))
chem <- analyze_data %>% select(,c(polyphenol:ECG,level))
table(chem$level)
chem$level <- factor(chem$level) %>% as.numeric()
chem$level <- ifelse(chem$level>1,1,0)
test_idx <- sample(1:138,size = 30)
train_idx <- not(1:138 %in% test_idx)
chem_test <- chem[test_idx,]
chem_train <- chem[train_idx,]
chem_test_y <- chem_test$level

```

# functions include

```{r}
simple_roc <- function(labels, scores){
  labels <- labels[order(scores, decreasing=TRUE)]
  data.frame(TPR=cumsum(labels)/sum(labels), FPR=cumsum(!labels)/sum(!labels), labels)
}
```

```{r}
importanceplot <- function(pp=and_rf) {
  i_scores <- varImp(pp, conditional=TRUE)
  #Gathering rownames in 'var'  and converting it to the factor
  #to provide 'fill' parameter for the bar chart.
  i_scores <- i_scores %>% tibble::rownames_to_column("var")
  i_scores$var<- i_scores$var %>% as.factor()

  #Plotting the bar and polar charts for comparing variables
  i_bar <- ggplot(data = i_scores) +
    geom_bar(
      stat = "identity",#it leaves the data without count and bin
      mapping = aes(x = var, y=Overall, fill = var),
      show.legend = FALSE,
      width = 1
    ) +
    labs(x = NULL, y = NULL)
  i_bar + coord_flip() + theme_minimal()
  }

```

```{r,include = FALSE}
cross_validation <- function(y_pred,ytest){
  y_pred <- ifelse(y_pred<=0.75,'B','C')
  ytest <- chem_test$level
  print(table(y_pred,ytest))
}
```

```{r}
rf_this <- function(y,dat,train, test,train_idx, test_idx) {
  model_formula <- formula(paste0(y,'~.'))
  xtrain <- model.matrix(model_formula, train)[,-1]
  ytrain <- train[y]
  ytest <- test[y]
  xtest <- model.matrix(model_formula, test)[,-1] %>%  as_tibble()%>% as.matrix()
  and_rf <- randomForest(model_formula, data = dat[train_idx,])
  print(and_rf)
  importanceplot(and_rf)
}
```

# data explore

season , tea type , cultivar are not balanced included

```{R}
ggplot(data=tea,aes(observe_ys,tea_kind,color=cultivar))+
  geom_point()+
  facet_wrap(~cultivar)
```

cutting height is not equal each time

```{R}
ggplot(data=tea,aes(observe_ys,cutting_height,color=cultivar))+
  geom_point()+
  facet_wrap(~cultivar)
```

some times seems linear but most of time is not , when split in to different cultivar

```{R}
ggplot(data=tea,aes(duration_oc,cutting_height,color=cultivar))+
  geom_point()+
  facet_wrap(~cultivar)
```

use lasso to choose var in weather and plant phenotypes

```{r}
train_idx <- sample(1:138,108)
test_idx <- !(1:138 %in%train_idx)
train <- numeric_dat[train_idx,]
test <- numeric_dat[test_idx,]
xtrain <- model.matrix(polyphenol~., train)[,-1]
ytrain <- train$polyphenol

ytest <- test$polyphenol
xtest <- model.matrix(polyphenol~., test)[,-1]
season7th <- 0
xtest <- cbind(xtest,season7th)
lambdas_to_try <- 10^seq(-3, 7, length.out = 100)
lasso_cv <- cv.glmnet(xtrain, ytrain, alpha = 1, lambda = lambdas_to_try)
plot(lasso_cv)
best_lambda_lasso <- lasso_cv$lambda.min
lasso_mod <- glmnet(xtrain, ytrain, alpha = 1, lambda = best_lambda_lasso)
print(lasso_mod)
```

use randomforest to estimate chem\*\~weather

1.polyphenol,acu_temp is most important

```{r}
rf_dat <- analyze_data %>% select(., c(polyphenol,cultivar,rain,acu_mean_temp,RH,Solar_rad_H,Solar_rad_MJM2))
rf_this('polyphenol',rf_dat,train,test,train_idx,test_idx)
```

2.FAA,acu_temp is most important

```{r}
rf_dat <- analyze_data %>% select(., c(FAA,cultivar,rain,acu_mean_temp,RH,Solar_rad_H,Solar_rad_MJM2))
rf_this('FAA',rf_dat,train,test,train_idx,test_idx)
```

Gallic_Acid,,acu_temp is most important

```{r}
rf_dat <- analyze_data %>% select(., c(Gallic_Acid,cultivar,rain,acu_mean_temp,RH,Solar_rad_H,Solar_rad_MJM2))
rf_this('Gallic_Acid',rf_dat,train,test,train_idx,test_idx)

```

GC,acu_temp is most important

```{r}
rf_dat <- analyze_data %>% select(., c(GC,cultivar,rain,acu_mean_temp,RH,Solar_rad_H,Solar_rad_MJM2))
rf_this('GC',rf_dat,train,test,train_idx,test_idx)

```

GCG,acu_temp is most important

```{r}
rf_dat <- analyze_data %>% select(., c(GCG,cultivar,rain,acu_mean_temp,RH,Solar_rad_H,Solar_rad_MJM2))
rf_this('GCG',rf_dat,train,test,train_idx,test_idx)

```

Catechin,acu_temp is most important

```{r}
rf_dat <- analyze_data %>% select(., c(Catechin,cultivar,rain,acu_mean_temp,RH,Solar_rad_H,Solar_rad_MJM2))
rf_this('Catechin',rf_dat,train,test,train_idx,test_idx)

```

# level\~chem

```{r}
pr_chem <- prcomp(scale(chem) )
biplot(pr_chem)
```

random forest ver.

```{r}
xtrain <- model.matrix(level~., train)[,-1]
ytrain <- train$level
ytest <- test$level
xtest <- model.matrix(level~., test)[,-1] %>%  as_tibble() %>% as.matrix()
data_level <- analyze_data %>% select(., c(polyphenol:total_catechins,level))
data_level$level <- factor(data_level$level)
and_rf <- randomForest(level~., data = data_level[train_idx,])
and_pred <- predict(and_rf,data_level[test_idx,])
table(data_level[test_idx,]$level,and_pred)
importanceplot(and_rf)

```

stepwise - logistic regression ver.

```{r}
# 1.建立空的線性迴歸(只有截距項)
null <- glm(level~1 ,data=chem_train,family=binomial())
full <- glm(level~. ,data=chem_train,family=binomial())

# 2.使用step()，一個一個把變數丟進去
forward.lm = step(null, 
                  # 從空模型開始，一個一個丟變數，
                  # 最大不會超過完整的線性迴歸
                  # (一定要加上界 upper=full，不可以不加) 
                  scope=list(lower=null, upper=full), 
                  direction="forward",
                  trace = 0)
print(forward.lm)
y_pred_forward <- predict(forward.lm,chem_test)
cross_validation(y_pred_forward,chem_test_y)

backward.lm = step(full, 
                  # 這裡可以加下界(lower=null)，也可以不加
                  scope = list(upper=full), 
                  direction="backward",
                  trace = 0) 
print(backward.lm)
y_pred_backward <- predict(backward.lm,chem_test)
cross_validation(y_pred_backward,chem_test_y)
```

```{R}
# lasso & ridge for chem ----
ridge = glmnet(x = as.matrix(chem[, -13]), 
               y = chem[, 13], 
               alpha = 0,
               family = "binomial")

lasso = glmnet(x = as.matrix(chem[, -13]), 
               y = chem[, 13], 
               alpha = 1,
               family = "binomial")

par(mfcol = c(1, 2)) # cool!!
plot(lasso, xvar='lambda', main="Lasso")
plot(ridge, xvar='lambda', main="Ridge")
```

```{R}
# 經由 cv 的手法，評估每個模型在不同 lambda 下 
# 的 cvm(mean cross-validated error)
cv.lasso = cv.glmnet(x = as.matrix(chem[, -13]), 
                     y = chem[, 13], 
                     alpha = 1,  # lasso
                     family = "binomial")
```

```{R}
# 評估每個模型的 cvm(mean cross-validated error)後
# 取最小 cvm 模型所對應的 lambda
best.lambda_lasso = cv.lasso$lambda.min

# 觀察哪些變數被挑選出來，其係數不為 0的那些
select.ind = which(coef(cv.lasso, s = "lambda.min") != 0)
select.ind = select.ind[-1]-1 # remove `Intercept` and 平移剩下的ind
select.ind # 第幾個變數是重要的 (不看 `Intercept`)
select.varialbes = colnames(chem)[select.ind]
select.varialbes
lasso.lm <- glm(level ~ ., family = 'binomial',data = chem_test[, c(select.varialbes, "level")])
lasso.test <- predict(lasso.lm,chem_test)
cross_validation(lasso.test,ytest)

# 經由 cv 的手法，評估每個模型在不同 lambda 下 
# 的 cvm(mean cross-validated error)
cv.ridge = cv.glmnet(x = as.matrix(chem[, -13]), 
                     y = chem[, 13], 
                     alpha = 0,  # lasso
                     family = "binomial")

# 評估每個模型的 cvm(mean cross-validated error)後
# 取最小 cvm 模型所對應的 lambda
best.lambda = cv.ridge$lambda.min
plot(lasso, xvar='lambda', main="Ridge")
abline(v=log(best.lambda), col="blue", lty=5.5 )
# 觀察哪些變數被挑選出來，其係數不為 0的那些
select.ind = which(coef(cv.lasso, s = "lambda.min") != 0)
select.ind = select.ind[-1]-1 # remove `Intercept` and 平移剩下的ind
select.ind # 第幾個變數是重要的 (不看 `Intercept`)
select.varialbes = colnames(chem)[select.ind]
select.varialbes
ridge.lm <- glm(level ~ ., family = 'binomial',data = chem_train[, c(select.varialbes, "level")])
ridge.test <- predict(ridge.lm,chem_test)
cross_validation(ridge.test,chem_test_y)
```

```{r}
chem <- analyze_data %>% select(,c(polyphenol:ECG,level))
chem$level <- factor(chem$level) %>% as.numeric()
chem$level <- ifelse(chem$level>1,1,0)

chem[,1:12] <- log(chem[,1:12])
chem[,6] <- ifelse(is.infinite(chem[,6]),-6,chem[,6])
test_idx <- sample(1:138,size = 30)
train_idx <- not(1:138 %in% test_idx)
chem_test <- chem[test_idx,]
chem_train <- chem[train_idx,]
chem_test_y <- chem_test$level
```

```{r}
# 1.建立空的線性迴歸(只有截距項)
null <- glm(level~1 ,data=chem_train,family=binomial())
full <- glm(level~. ,data=chem_train,family=binomial())

# 2.使用step()，一個一個把變數丟進去
forward.lm = step(null, 
                  # 從空模型開始，一個一個丟變數，
                  # 最大不會超過完整的線性迴歸
                  # (一定要加上界 upper=full，不可以不加) 
                  scope=list(lower=null, upper=full), 
                  direction="forward")
backward.lm = step(full, 
                  # 這裡可以加下界(lower=null)，也可以不加
                  scope = list(upper=full), 
                  direction="backward")  

summary(forward.lm)

summary(backward.lm)

y_pred_forward <- predict(forward.lm,chem_test)
cross_validation(y_pred_forward,chem_test_y)
y_pred_backward <- predict(backward.lm,chem_test)
cross_validation(y_pred_backward,chem_test_y)
```

```{R}
# lasso & ridge for chem ----
ridge = glmnet(x = as.matrix(chem[, -13]), 
               y = chem[, 13], 
               alpha = 0,
               family = "binomial")

lasso = glmnet(x = as.matrix(chem[, -13]), 
               y = chem[, 13], 
               alpha = 1,
               family = "binomial")

par(mfcol = c(1, 2)) # cool!!
plot(lasso, xvar='lambda', main="Lasso")
plot(ridge, xvar='lambda', main="Ridge")

# 經由 cv 的手法，評估每個模型在不同 lambda 下 
# 的 cvm(mean cross-validated error)
cv.lasso = cv.glmnet(x = as.matrix(chem[, -13]), 
                     y = chem[, 13], 
                     alpha = 1,  # lasso
                     family = "binomial")

# 評估每個模型的 cvm(mean cross-validated error)後
# 取最小 cvm 模型所對應的 lambda
best.lambda_lasso = cv.lasso$lambda.min


plot(lasso, xvar='lambda', main="Lasso")
abline(v=log(best.lambda_lasso), col="blue", lty=5.5 )
# 觀察哪些變數被挑選出來，其係數不為 0的那些
coef(cv.lasso, s = "lambda.min")
select.ind = which(coef(cv.lasso, s = "lambda.min") != 0)
select.ind = select.ind[-1]-1 # remove `Intercept` and 平移剩下的ind
select.ind # 第幾個變數是重要的 (不看 `Intercept`)
select.varialbes = colnames(chem)[select.ind]
select.varialbes
lasso.lm <- glm(level ~ ., family = 'binomial',data = chem_train[, c(select.varialbes, "level")])
print(lasso.lm)
summary(lasso.lm)
lasso.test <- predict(lasso.lm,chem_test)
cross_validation(lasso.test,chem_test_y)
```

```{R}
# 經由 cv 的手法，評估每個模型在不同 lambda 下 
# 的 cvm(mean cross-validated error)
cv.ridge = cv.glmnet(x = as.matrix(chem[, -13]), 
                     y = chem[, 13], 
                     alpha = 0,  # lasso
                     family = "binomial")

# 評估每個模型的 cvm(mean cross-validated error)後
# 取最小 cvm 模型所對應的 lambda
best.lambda = cv.ridge$lambda.min

plot(lasso, xvar='lambda', main="Ridge")
abline(v=log(best.lambda), col="blue", lty=5.5 )
# 觀察哪些變數被挑選出來，其係數不為 0的那些
coef(cv.lasso, s = "lambda.min")
select.ind = which(coef(cv.lasso, s = "lambda.min") != 0)
select.ind = select.ind[-1]-1 # remove `Intercept` and 平移剩下的ind
select.ind # 第幾個變數是重要的 (不看 `Intercept`)
select.varialbes = colnames(chem)[select.ind]
select.varialbes
ridge.lm <- glm(level ~ ., family = 'binomial',data = chem_train[, c(select.varialbes, "level")])
print(ridge.lm)
summary(ridge.lm)
ridge.test <- predict(ridge.lm,chem_test)
cross_validation(ridge.test,chem_test_y)
```

```{r,include = FALSE}
scatter_hist_plot <- function(dat) {
  n = length(colnames(dat))
  for (i in 1:n) 
    {
     for(j in 1:n)
      {
        if(i==j)
        {
          test <- ggplot(data=dat,aes(get(colnames(dat[i]))))+
            geom_histogram(bins = 30,fill = 'darkgreen')+
            labs(x=paste(i,colnames(dat[i]),y=NULL))
        }
        else
        {
          test <- ggplot(data=dat,aes(get(colnames(dat[i])),get(colnames(dat[j]))),colour='green')+
            geom_point()+
            labs(x=paste(i,colnames(dat[i])),y=paste(j,colnames(dat[j])))
        }
        print(test,vp=viewport(width=1/n, 
                               height=1/n,
                               x=((i-1)%%n)/n+1/(2*n),
                               y=((j-1)%%n)/n+1/(2*n)
                               )
              )
      }
    }
}


```

```{r}
test_idx <- sample(1:138,size = 30)
train_idx <- not(1:138 %in% test_idx)
weather_test <- weather[test_idx,]
weather_train <- weather[train_idx,]
weather_test_y <- weather_test$caffeine  
```

```{r}
# 1.建立空的線性迴歸(只有截距項)
null <- lm(caffeine  ~1 ,data=weather_train)
full <- lm(caffeine  ~. ,data=weather_train)

# 2.使用step()，一個一個把變數丟進去
forward.lm = step(null, 
                  # 從空模型開始，一個一個丟變數，
                  # 最大不會超過完整的線性迴歸
                  # (一定要加上界 upper=full，不可以不加) 
                  scope=list(lower=null, upper=full), 
                  direction="forward")
backward.lm = step(full, 
                  # 這裡可以加下界(lower=null)，也可以不加
                  scope = list(upper=full), 
                  direction="backward")  

summary(forward.lm)
summary(backward.lm)

inter <- lm(level  ~. +caffeine*Gallic_Acid*GCG,data=chem_train)
inter.b.lm = step(inter, 
                  # 從空模型開始，一個一個丟變數，
                  # 最大不會超過完整的線性迴歸
                  # (一定要加上界 upper=full，不可以不加) 
                  scope=list(lower=null, upper=inter), 
                  direction="backward")
summary(inter.b.lm)
y_pred_forward <- predict(forward.lm,weather_test)
accuracy(y_pred_forward,weather_test_y)
y_pred_backward <- predict(backward.lm,weather_test)
accuracy(y_pred_backward,weather_test_y)

```

5/13 report

```{r}

```
